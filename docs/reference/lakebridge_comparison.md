# Lakebridge vs. Informatica Modernization Accelerator - Comparative Analysis

## Executive Summary

This document compares **Databricks Lakebridge** (a Databricks Labs toolkit) with our **Informatica Modernization Accelerator** to identify:
1. **Learning opportunities** from Lakebridge
2. **Areas where our solution is superior**
3. **Potential improvements** we can adopt

---

## 1. Solution Overview Comparison

### Lakebridge
- **Focus**: Multi-platform migration to Databricks (SQL Server, Oracle, Teradata, Snowflake, Netezza, DataStage, Informatica)
- **Architecture**: Three-phase approach (Assessment ‚Üí Transpilation ‚Üí Reconciliation)
- **Transpilers**: BladeBridge (rules-based), Morpheus (next-gen), Switch (LLM-powered)
- **Target**: Databricks SQL and Databricks Notebooks
- **Approach**: Direct SQL/ETL conversion with minimal intermediate representation

### Our Solution
- **Focus**: Deep Informatica modernization with AI augmentation
- **Architecture**: Multi-phase approach (Source ‚Üí Canonical Model ‚Üí Target Code ‚Üí Assessment ‚Üí Reconciliation)
- **Canonical Model**: Technology-neutral JSON representation as single source of truth
- **Storage**: Dual-mode (file-based + optional Neo4j graph database)
- **Target**: PySpark, Delta Live Tables (DLT), SQL, orchestration (Airflow, Prefect, Databricks Workflows)
- **Approach**: Canonical model-first with optional graph database storage and multi-agent AI enhancement
- **LLM Support**: OpenAI, Azure OpenAI, Local LLM (Ollama/vLLM) with intelligent fallback

---

## 2. Key Differences

### 2.1 Architecture Philosophy

| Aspect | Lakebridge | Our Solution |
|--------|-----------|--------------|
| **Intermediate Representation** | Direct conversion (SQL ‚Üí SQL) | Canonical Model (XML ‚Üí JSON ‚Üí Code) |
| **Storage** | File-based | File-based + Optional Neo4j Graph Database |
| **Lineage** | Limited (SQL-based) | Comprehensive (Graph-based, field-level) when graph enabled |
| **AI Integration** | LLM transpiler (Switch) | Multi-agent AI system (11 specialized agents) with fallback |
| **LLM Providers** | Single provider | Multiple providers (OpenAI, Azure, Local) with intelligent fallback |
| **Workflow Awareness** | Basic (workflow ‚Üí session ‚Üí mapping) | Deep (workflow ‚Üí worklet ‚Üí session ‚Üí mapping with relationships) |
| **Pre-Migration Assessment** | ‚úÖ Profiler + Analyzer | ‚úÖ Profiler + Analyzer + Wave Planner + TCO Calculator |
| **Post-Migration Reconciliation** | ‚úÖ Comprehensive | ‚úÖ Comprehensive (Count, Hash, Threshold, Sampling) |

### 2.2 Informatica Support

| Feature | Lakebridge | Our Solution |
|---------|-----------|--------------|
| **Mapping Parsing** | ‚úÖ Basic (via BladeBridge) | ‚úÖ Comprehensive (dedicated parsers) |
| **Workflow Parsing** | ‚úÖ Basic | ‚úÖ Comprehensive (workflow, worklet, session) |
| **Transformation Types** | Limited (SQL-focused) | Extensive (Expression, Lookup, Aggregator, Router, Union, etc.) |
| **Expression Translation** | Basic SQL conversion | Advanced AST-based translation with Informatica function mapping |
| **Workflow Orchestration** | Basic JSON output | Full orchestration (Airflow DAGs, Prefect, Databricks Workflows) |
| **Canonical Model** | ‚ùå No intermediate model | ‚úÖ Rich canonical model with lineage |

---

## 3. Current Feature Comparison

### 3.1 Assessment Phase (Pre-Migration)

**Lakebridge:**
- **Profiler**: Connects to source SQL environments, profiles workloads, reports size/complexity/feature usage
- **Analyzer**: Scans SQL/orchestration code, identifies patterns, estimates migration effort, highlights blockers
- **TCO Impact Analysis**: Estimates cost savings and runtime impact on Databricks

**Our Solution:**
- **Profiler** (`src/assessment/profiler.py`): Profiles Informatica repository stored in Neo4j graph, calculates complexity metrics, feature usage patterns
- **Analyzer** (`src/assessment/analyzer.py`): Analyzes Informatica-specific patterns (lookup cache usage, partitioning strategies, custom functions), identifies migration blockers
- **Wave Planner** (`src/assessment/wave_planner.py`): Generates migration wave recommendations based on dependencies and complexity
- **TCO Calculator** (`src/assessment/tco_calculator.py`): Compares Informatica licensing costs vs. Databricks compute costs, estimates runtime improvements, provides ROI analysis
- **Report Generator** (`src/assessment/report_generator.py`): Generates comprehensive assessment reports

**Requirements**: Graph store must be enabled (`ENABLE_GRAPH_STORE=true`) for assessment features

### 3.2 Reconciliation (Post-Migration Validation)

**Lakebridge:**
- **Reconciler**: Compares source and Databricks datasets
- **Handles Live Systems**: Works even when both environments are active
- **Data Validation**: Detects mismatches, missing records, data integrity issues
- **Aggregate Reconciliation**: Supports count, hash, threshold, and sampling comparisons

**Our Solution:**
- **Reconciliation Engine** (`src/reconciliation/recon_engine.py`): Orchestrates reconciliation between Informatica source and Databricks target
- **Data Comparator** (`src/reconciliation/data_comparator.py`): Supports multiple comparison methods:
  - Count-based reconciliation
  - Hash-based reconciliation (row-level)
  - Threshold-based reconciliation (tolerance levels)
  - Sampling-based reconciliation (for large datasets)
- **Live System Support**: Works even when both environments are active
- **Incremental Reconciliation**: Supports phased migrations
- **Report Generator** (`src/reconciliation/report_generator.py`): Generates reconciliation reports (JSON, HTML) with drill-down capabilities
- **Integration**: Integrated with code generation pipeline and API endpoints

### 3.3 Multi-Platform Support

**Lakebridge:**
- Pluggable transpiler architecture (BladeBridge, Morpheus, Switch)
- Support for multiple source platforms (SQL Server, Oracle, Teradata, Snowflake, Netezza, DataStage, Informatica)
- Unified CLI interface for all platforms

**Our Solution:**
- **Current Focus**: Informatica-only (deep specialization)
- **Architecture**: Canonical model structure is platform-agnostic, enabling future multi-platform support
- **Future**: Can extend to DataStage, SSIS, Talend using same canonical model architecture

### 3.4 CLI and Developer Experience

**Lakebridge:**
- Integrated with Databricks CLI (`databricks labs lakebridge`)
- Simple, parameterized commands
- Configuration file support
- Error logging and reporting

**Our Solution:**
- **Unified CLI** (`src/cli/main.py`): Comprehensive command structure
- **Configuration Support**: YAML/JSON configuration files (`src/cli/config.py`)
- **Error Reporting**: Categorized error reporting with recovery suggestions
- **Progress Indicators**: Visual progress indicators for long-running operations
- **API Integration**: RESTful API (`src/api/`) for programmatic access

### 3.5 Validation and Testing

**Lakebridge:**
- SQL validation against Databricks Unity Catalog
- Error categorization (analysis, parsing, validation, generation)
- Comprehensive error logging

**Our Solution:**
- **Databricks Validator** (`src/validation/databricks_validator.py`): Validates generated code against Databricks Unity Catalog and Delta Lake
- **Test Data Validator** (`src/validation/test_data_validator.py`): Validates generated code against sample data
- **Automated Test Generation**: Generates unit tests (PySpark, SQL) automatically
- **Integration Testing Framework** (`src/testing/integration_test_framework.py`): End-to-end testing capabilities
- **Error Categorization**: 28 error categories with 5 severity levels
- **Error Recovery**: Automatic recovery strategies (retry, skip, use defaults)

---

## 4. Where Our Solution is Superior

### 4.1 Canonical Model Architecture

**Our Advantage:**
- **Technology-Neutral Representation**: Canonical model abstracts away Informatica specifics
- **Single Source of Truth**: All generators work from the same model
- **Regeneration Capability**: Code can be regenerated without re-parsing XML
- **Extensibility**: Easy to add new transformation types or metadata

**Lakebridge Limitation:**
- Direct conversion approach means regenerating requires re-parsing source
- No intermediate representation for cross-platform analysis

### 4.2 Graph Database Storage (Optional)

**Our Advantage:**
- **Neo4j Integration** (when `ENABLE_GRAPH_STORE=true`): Complete graph storage of components and relationships
- **Dual-Mode Storage**: Can operate in file-based mode (default) or graph-first mode
- **Cross-Mapping Lineage**: Query relationships across mappings, workflows, sessions (when graph enabled)
- **Impact Analysis**: Understand downstream effects of changes (when graph enabled)
- **Pattern Discovery**: Find reusable patterns across mappings (when graph enabled)
- **Rich Metadata**: Store file metadata, code metadata, quality scores
- **Assessment Features**: Profiler, Analyzer, Wave Planner require graph store

**Lakebridge Limitation:**
- File-based storage only
- Limited cross-mapping analysis capabilities

**Note**: Our solution works without Neo4j for basic functionality, but graph store enables advanced features

### 4.3 AI and Intelligence Layer

**Our Advantage:**
- **11 Specialized AI Agents** (`ai_agents/`):
  - Rule Explainer Agent: Explains Informatica expressions in business terms
  - Mapping Summary Agent: Generates comprehensive narrative summaries
  - Risk Detection Agent: Identifies potential issues and risks
  - Transformation Suggestion Agent: Suggests optimizations and modernizations
  - Code Fix Agent: Automatically fixes errors in generated code
  - Impact Analysis Agent: Analyzes downstream impact of changes
  - Mapping Reconstruction Agent: Reconstructs mappings from partial information
  - Workflow Simulation Agent: Simulates execution and identifies bottlenecks
  - Model Enhancement Agent: Enhances canonical models with AI insights
  - Model Validation Agent: Validates canonical models
  - Code Review Agent: Reviews and improves generated code
- **LLM Provider Support**: OpenAI, Azure OpenAI, Local LLM (Ollama/vLLM)
- **Intelligent Fallback**: OpenAI ‚Üí Local LLM ‚Üí Error (graceful degradation)
- **Deep Reasoning**: Agents analyze canonical model structure, not just code
- **Proactive Suggestions**: AI suggests optimizations, identifies risks, explains logic
- **Response Caching**: LLM responses are cached for performance

**Lakebridge Limitation:**
- Single LLM transpiler (Switch) for code conversion
- Limited AI reasoning beyond code generation
- Single provider support

### 4.4 Informatica-Specific Deep Understanding

**Our Advantage:**
- **Comprehensive Parsing**: Dedicated parsers for workflow, worklet, session, mapping
- **Transformation Coverage**: Support for all Informatica transformation types
- **Expression Engine**: AST-based translation of Informatica expressions
- **Workflow Orchestration**: Full workflow ‚Üí worklet ‚Üí session ‚Üí mapping hierarchy
- **Informatica Function Mapping**: Comprehensive translation of Informatica functions to PySpark/SQL

**Lakebridge Limitation:**
- Informatica support is one of many platforms (less depth)
- Focus on SQL conversion rather than ETL transformation logic
- Limited workflow orchestration generation

### 4.5 Code Generation Quality

**Our Advantage:**
- **Multiple Target Formats**: PySpark, DLT, SQL, orchestration (Airflow, Prefect, Databricks)
- **Code Quality Checks**: Automated quality scoring and recommendations
- **Best Practices**: Generates code following Databricks best practices
- **Documentation**: Auto-generates mapping specs, READMEs, workflow documentation
- **Workflow-Aware Structure**: Code organized by workflow ‚Üí task ‚Üí transformation

**Lakebridge Limitation:**
- Primarily SQL/notebook generation
- Limited orchestration code generation
- Less focus on code quality and best practices

### 4.6 User Interface and Visualization

**Our Advantage:**
- **Rich Web UI**: React-based visualization dashboard
- **Canonical Model Explorer**: Interactive tree view of workflows ‚Üí tasks ‚Üí transformations
- **Code Repository View**: File tree browser for generated code
- **Component View**: Overview of all components with metadata
- **Graph Explorer**: Visual lineage and relationship exploration
- **Code View**: Navigate and view generated code with quality scores

**Lakebridge Limitation:**
- CLI-focused (no web UI)
- Limited visualization capabilities

### 4.7 Lineage and Impact Analysis

**Our Advantage:**
- **Field-Level Lineage**: Track data flow at column level
- **Transformation-Level Lineage**: Understand transformation dependencies
- **Workflow-Level Lineage**: Complete workflow execution graph
- **Graph Queries**: Complex queries for impact analysis, dependency tracking
- **Visual Lineage**: Mermaid diagrams and graph visualizations

**Lakebridge Limitation:**
- SQL-based lineage (limited to SQL statements)
- No field-level lineage
- Limited impact analysis capabilities

---

## 5. Feature Comparison Matrix

| Feature | Lakebridge | Our Solution | Winner |
|---------|-----------|-------------|--------|
| **Pre-Migration Assessment** | ‚úÖ Profiler + Analyzer | ‚úÖ Complete (Profiler, Analyzer, Wave Planner, TCO Calculator) | üèÜ Our Solution |
| **Post-Migration Reconciliation** | ‚úÖ Comprehensive | ‚úÖ Complete (Count, Hash, Threshold, Sampling methods) | üèÜ Tie |
| **Multi-Platform Support** | ‚úÖ 7+ platforms | ‚ö†Ô∏è Informatica only | üèÜ Lakebridge |
| **Canonical Model** | ‚ùå No | ‚úÖ Rich model | üèÜ Our Solution |
| **Graph Database Storage** | ‚ùå No | ‚úÖ Neo4j | üèÜ Our Solution |
| **AI Intelligence** | ‚ö†Ô∏è LLM transpiler | ‚úÖ 11 specialized agents | üèÜ Our Solution |
| **Informatica Depth** | ‚ö†Ô∏è Basic | ‚úÖ Comprehensive | üèÜ Our Solution |
| **Workflow Orchestration** | ‚ö†Ô∏è Basic JSON | ‚úÖ Airflow/Prefect/Databricks | üèÜ Our Solution |
| **Code Quality** | ‚ö†Ô∏è Basic | ‚úÖ Quality checks + scoring + Databricks validation | üèÜ Our Solution |
| **User Interface** | ‚ùå CLI only | ‚úÖ Rich web UI | üèÜ Our Solution |
| **Lineage** | ‚ö†Ô∏è SQL-based | ‚úÖ Field-level graph | üèÜ Our Solution |
| **Expression Translation** | ‚ö†Ô∏è Basic | ‚úÖ AST-based | üèÜ Our Solution |
| **Documentation** | ‚ö†Ô∏è Limited | ‚úÖ Auto-generated specs | üèÜ Our Solution |
| **CLI Experience** | ‚úÖ Integrated | ‚úÖ Unified CLI with config support | üèÜ Tie |
| **Validation** | ‚úÖ SQL validation | ‚úÖ Comprehensive (Databricks validation, test data validation, automated test generation) | üèÜ Our Solution |
| **Testing Framework** | ‚ö†Ô∏è Basic | ‚úÖ Complete (Test generation, validation, integration testing) | üèÜ Our Solution |
| **TCO Analysis** | ‚ö†Ô∏è Basic | ‚úÖ Complete (Cost comparison, ROI, runtime estimation) | üèÜ Our Solution |

---

## 6. Current Architecture Details

### 6.1 Storage Architecture

**Our Solution:**
- **File-Based Mode** (Default): JSON files in `versions/` directory
- **Graph-First Mode** (Optional): Neo4j as primary storage with JSON fallback
- **Configuration**: Set `ENABLE_GRAPH_STORE=true` and `GRAPH_FIRST=true` in `.env`
- **Graph Store Features**: Requires Neo4j 5.15+ (see `docs/getting-started/setup_neo4j.md`)

### 6.2 LLM Configuration

**Our Solution:**
- **Multiple Providers**: OpenAI (default), Azure OpenAI, Local LLM (Ollama/vLLM)
- **Configuration**: Set `LLM_PROVIDER` in `.env` file
- **Fallback Logic**: 
  - OpenAI ‚Üí Local LLM (if OpenAI fails)
  - Local LLM ‚Üí OpenAI (if Local fails)
  - Azure ‚Üí OpenAI (if Azure fails)
- **Local LLM**: Default Ollama at `http://localhost:11434` with model `llama3`
- **Caching**: LLM responses are cached for performance

### 6.3 Component Structure

**Our Solution:**
- **Parsers**: `src/parser/` (Mapping, Workflow, Session, Worklet, Mapplet)
- **Normalizer**: `src/normalizer/` (Canonical model creation, lineage, SCD detection)
- **Translator**: `src/translator/` (AST-based expression translation)
- **Generators**: `src/generators/` (PySpark, DLT, SQL, Specs, Tests, Orchestration)
- **Graph Store**: `src/graph/` (Neo4j integration, queries, sync)
- **Assessment**: `src/assessment/` (Profiler, Analyzer, Wave Planner, TCO Calculator)
- **Reconciliation**: `src/reconciliation/` (Data comparison, reports)
- **Validation**: `src/validation/` (Databricks validation, test data validation)
- **AI Agents**: `ai_agents/` (11 specialized agents)
- **LLM**: `src/llm/` (Manager, clients, prompt templates)
- **API**: `src/api/` (FastAPI REST endpoints)
- **CLI**: `src/cli/` (Unified command interface)

---

## 7. Configuration and Setup

### 7.1 Environment Configuration

**Our Solution** (via `.env` file):
```bash
# LLM Configuration
LLM_PROVIDER=local  # Options: openai, azure, local
OPENAI_API_KEY=sk-...  # For OpenAI
OLLAMA_URL=http://localhost:11434  # For local LLM
OLLAMA_MODEL=llama3  # For local LLM

# Neo4j Configuration (optional)
ENABLE_GRAPH_STORE=true
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password
GRAPH_FIRST=false  # Set to true for graph-first mode

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
```

### 7.2 Setup Requirements

**Basic Operation** (No external dependencies):
- Python 3.10+
- Dependencies from `requirements.txt`
- File-based storage (default)

**With LLM Features**:
- OpenAI API key OR
- Local Ollama server running

**With Graph Store Features**:
- Neo4j 5.15+ (Docker recommended)
- See `docs/getting-started/setup_neo4j.md` for setup

**With Assessment Features**:
- Requires graph store to be enabled
- Neo4j must be running and accessible

---

## 8. Conclusion

### Our Solution's Unique Strengths

1. **Canonical Model Architecture**: Technology-neutral representation enables regeneration and extensibility
2. **Graph Database Storage**: Rich relationships and lineage enable deep analysis
3. **AI Intelligence**: 11 specialized agents provide comprehensive reasoning
4. **Informatica Depth**: Deep understanding of Informatica-specific features
5. **User Experience**: Rich web UI for visualization and exploration

### Feature Parity with Lakebridge

**Assessment Phase**: ‚úÖ Pre-migration profiling and analysis (Profiler, Analyzer, Wave Planner, TCO Calculator)
**Reconciliation**: ‚úÖ Post-migration data validation (Count, Hash, Threshold, Sampling methods)
**CLI Experience**: ‚úÖ Unified CLI with configuration support
**Validation**: ‚úÖ Comprehensive validation (Databricks, test data, automated tests)
**Multi-Platform**: ‚ö†Ô∏è Informatica-only (canonical model architecture enables future extension)

### Competitive Positioning

**Our Solution is Superior For:**
- Deep Informatica modernization projects
- Organizations needing AI-augmented analysis (11 specialized agents)
- Complex workflow orchestration requirements
- Field-level lineage and impact analysis (when graph enabled)
- Rich visualization and exploration (web UI)
- Pre-migration assessment with TCO analysis (requires graph store)
- Post-migration reconciliation
- Comprehensive testing and validation
- Multi-provider LLM support with fallback
- Canonical model architecture for regeneration

**Lakebridge is Superior For:**
- Multi-platform migrations (SQL Server, Oracle, Teradata, Snowflake, etc.)
- Quick SQL-to-SQL conversions
- Organizations already using Databricks CLI
- Simple file-based workflows without graph database requirements

### Current Capabilities Summary

**Our solution includes:**

1. **Core Modernization Pipeline**
   - Comprehensive Informatica XML parsing (Workflow, Worklet, Session, Mapping, Mapplet)
   - Canonical model creation with lineage and SCD detection
   - AST-based expression translation (100+ Informatica functions)
   - Multi-format code generation (PySpark, DLT, SQL, Specs, Tests, Orchestration)
   - Source Qualifier handling (SQ transformations properly converted to DataFrame reads)
   - Comprehensive validation (canonical model validation, generated code validation)

2. **Assessment and Analysis** (requires graph store)
   - Pre-migration profiling and complexity analysis
   - Migration blocker identification
   - Migration wave planning
   - TCO calculator with ROI analysis

3. **Reconciliation and Validation**
   - Post-migration data reconciliation (count, hash, threshold, sampling)
   - Databricks code validation
   - Automated test generation
   - Integration testing framework

4. **AI and Intelligence**
   - 11 specialized AI agents
   - Multi-provider LLM support with fallback
   - Response caching for performance
   - AI review status tracking (separate from quality scores)
   - Automated code fixing based on AI review

5. **Storage and Infrastructure**
   - File-based storage (default)
   - Optional Neo4j graph database
   - Dual-mode operation (file-first or graph-first)

6. **Developer Experience**
   - Unified CLI with configuration support
   - RESTful API for programmatic access
   - Rich web UI for visualization
   - Comprehensive error handling and recovery

**Differentiators:**
- Deep Informatica specialization vs. multi-platform breadth
- Canonical model architecture enables regeneration and extensibility
- Graph database enables advanced analysis (when enabled)
- Multi-agent AI system vs. single LLM transpiler
- Rich web UI vs. CLI-only

---

## Appendix: Technical Deep Dive

### A.1 Canonical Model Comparison

**Lakebridge**: No canonical model - direct SQL conversion
```python
# Lakebridge approach
SQL (Source) ‚Üí Transpiler ‚Üí SQL (Target)
```

**Our Solution**: Canonical model as intermediate representation
```python
# Our approach
XML (Informatica) ‚Üí Parser ‚Üí Canonical Model (JSON) ‚Üí Generator ‚Üí Code (PySpark/DLT/SQL)
```

**Advantage**: Our approach enables:
- Regeneration without re-parsing
- Multiple target formats from same model
- Cross-platform analysis
- AI enhancement of model

### A.2 Storage Architecture Comparison

**Lakebridge**: File-based
```
output/
  ‚îú‚îÄ‚îÄ mapping1.py
  ‚îú‚îÄ‚îÄ mapping2.py
  ‚îî‚îÄ‚îÄ workflow.json
```

**Our Solution**: Dual-mode storage (file-based default, optional graph)
```
File-Based Mode (Default):
  - Generated code files
  - Canonical model JSON in versions/
  - No external dependencies

Graph-First Mode (Optional, when ENABLE_GRAPH_STORE=true):
  Neo4j Graph:
    - Workflow nodes
    - Session nodes
    - Mapping nodes
    - Relationships (CONTAINS, EXECUTES, etc.)
    - Metadata (file paths, quality scores)
  
  Filesystem:
    - Generated code files
    - Canonical model JSON (backup)
```

**Advantage**: Our approach enables:
- Works without Neo4j for basic functionality
- Optional graph enables complex relationship queries
- Impact analysis (when graph enabled)
- Pattern discovery (when graph enabled)
- Rich metadata storage (when graph enabled)
- Assessment features require graph store

### A.3 AI Integration Comparison

**Lakebridge**: Single LLM transpiler (Switch)
- Converts SQL/ETL to Databricks notebooks
- Limited reasoning beyond conversion

**Our Solution**: Multi-agent AI system with multi-provider LLM support
- 11 specialized agents for different tasks
- Deep analysis of canonical model
- Proactive suggestions and optimizations
- Code review and fixing
- Multiple LLM providers (OpenAI, Azure, Local) with intelligent fallback
- Response caching for performance

**Advantage**: Our approach provides:
- Comprehensive analysis
- Proactive recommendations
- Code quality improvements
- Business logic explanation
- Resilience through fallback mechanisms
- Works without external LLM (local mode)

---

*Document created: 2025-12-02*
*Last updated: 2025-12-04*
*Status: Current state comparison - reflects actual implementation as of December 2025*

## Recent Enhancements (December 2024)

### Code Generation Improvements
- **Source Qualifier Support**: Source Qualifiers (SQ_*) are now properly handled in PySpark generation, creating DataFrame reads from SQL queries
- **Joiner Enhancement**: Joiners now correctly use connector information to identify source DataFrames
- **Validation Updates**: Code validation recognizes Source Qualifiers as `df_SQ_NAME` patterns in generated code

### Validation Enhancements
- **Transformation Type Recognition**: CUSTOM_TRANSFORMATION and STORED_PROCEDURE are now recognized as valid transformation types
- **Connector Validation**: Improved validation logic to handle Source Qualifiers and mapplet instances correctly
- **Comprehensive Testing**: Field-level lineage, parser tests, and semantic tag detection tests added

### AI Review Tracking
- **Separate Metrics**: AI review status (`ai_reviewed`, `ai_review_score`, `ai_fixed`) tracked separately from code quality scores
- **Component Health UI**: Enhanced UI shows AI review metrics alongside quality scores
- **Complete Workflow**: Canonical Model ‚Üí Code Generation ‚Üí Quality Check ‚Üí AI Review ‚Üí AI Fix (if needed)

